### ğŸ¥¸ğŸ—ï¸ğŸ“ˆ Fundamentos de ETL - Curso Power BI.

- Projeto desenvolvido como prÃ¡tica dos **Fundamentos de ETL** e automaÃ§Ã£o de ingestÃ£o de dados, utilizando Python para conectar diferentes fontes e simular um fluxo completo de coleta, transformaÃ§Ã£o e anÃ¡lise de dados.

![OracleTrainingInOhioAwsCertificationTrainingInOhioGIF](https://github.com/user-attachments/assets/acf16631-f0d9-41a8-872a-dfe29907b545)


---

## ğŸš€ Fundamentos de ETL

**ETL (Extract, Transform, Load)** Ã© um processo fundamental em engenharia e anÃ¡lise de dados. Envolve:

- **ExtraÃ§Ã£o (Extract)**: coleta de dados de mÃºltiplas fontes (bancos, APIs, arquivos, filas).
- **TransformaÃ§Ã£o (Transform)**: limpeza, normalizaÃ§Ã£o e preparaÃ§Ã£o dos dados.
- **Carga (Load)**: envio dos dados para uma base final (banco de dados, data warehouse, etc.).

Este projeto aplica esse conceito com foco em automaÃ§Ã£o, orquestraÃ§Ã£o e organizaÃ§Ã£o de dados para anÃ¡lises futuras.

---

## ğŸ§© Fontes de Dados Utilizadas

O pipeline se conecta a **4 fontes distintas**:

| Fonte             | Tipo                     | Tecnologia         | DescriÃ§Ã£o rÃ¡pida                             |
|-------------------|--------------------------|--------------------|----------------------------------------------|
| Legado            | Arquivo estruturado      | `.txt` (simulado)  | Simula um sistema antigo com layout fixo     |
| Banco SQL         | Relacional               | PostgreSQL         | Consulta registros diretamente via SQL       |
| Fila de Mensagens | AssÃ­ncrona               | RabbitMQ           | Consome mensagens em tempo real              |
| Nuvem             | Armazenamento em nuvem   | Azure Blob Storage | Faz download de arquivos de containers Azure |

---

## âš™ï¸ Funcionalidades

### âœ… IngestÃ£o Agendada
O pipeline Ã© executado automaticamente em intervalos definidos usando o pacote `schedule`.

### âœ… Interface CLI (linha de comando)

Execute partes do pipeline diretamente pelo terminal:

# Executar fonte legado
python main.py --legado

# Executar SQL + Azure
python main.py --sql --azure

### âœ… Interface Web (opcional)
Execute via navegador com Streamlit:
streamlit run app.py

### ğŸ’¡ Insights Esperados

* Facilitar a integraÃ§Ã£o de diferentes fontes de dados.

* Automatizar a coleta de dados brutos para futuras anÃ¡lises.

* Aprimorar a compreensÃ£o de pipelines ETL no contexto real.

* Criar base para geraÃ§Ã£o de relatÃ³rios, painÃ©is ou Machine Learning.

### ğŸ“Š Resultado Esperado

* Ao final da execuÃ§Ã£o: Os dados extraÃ­dos de todas as fontes estarÃ£o disponÃ­veis em arquivos .csv ou estruturas de DataFrame.

* Logs de execuÃ§Ã£o serÃ£o armazenados e exibidos no terminal.

* Os dados estarÃ£o prontos para ser conectado a ferramentas de BI ou mÃ³dulos de transformaÃ§Ã£o/anÃ¡lise.

### ğŸ› ï¸ Tecnologias e Bibliotecas

- Python 3.10+

- Pandas

- Psycopg2

- Pika (RabbitMQ)

- Azure Blob Storage SDK

- Schedule

### ğŸ“ Estrutura do Projeto
ETL-DIO-CURSO-POWERBI/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.yaml
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ pipeline.log
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ dados_legado.txt
â”‚   â””â”€â”€ baixado.csv
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ legado.py
â”‚   â”œâ”€â”€ postgres.py
â”‚   â”œâ”€â”€ azure_blob.py
â”‚   â”œâ”€â”€ fila.py
â”‚   â””â”€â”€ scheduler.py
â”œâ”€â”€ main.py
â””â”€â”€ app.py  # opcional (interface web)

### ğŸ‘¨â€ğŸ’» Autor
Raphael Travassos 
Analista de Dados â€¢ Projeto prÃ¡tico para a DIO
